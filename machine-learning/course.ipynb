{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Machine learning is the process whereby machines are given the ability to learn to make decisions from data without being explicity programmed.\n",
    "* <b>`Unsupervised Learning`</b> is the process of uncovering hidden patterns and structures from unlabeled data.\n",
    "    * Example grouping customers into distinct categories (`known as clustering`) based on various criteria like shopping behavior, most bought products etc \n",
    "* <b>`Supervised Learning`</b> is type of machine learning where the values to be predicted are already known and model is build to predict target values of unseen given data, given the features\n",
    "    * Uses features to predict the value of target variable\n",
    "\n",
    "## Types of Supervised Learning\n",
    "* <b>`Classification`</b> is used to predict the label, or cateogry of an observation. The target variable consists of categories\n",
    "    * Example cat or dog, spam email classification\n",
    "* <b>`Regression`</b> is used to predict continuous values\n",
    "    * Example property price prediction, stock forecasting\n",
    "\n",
    "### Naming Convention\n",
    "* `Feature` or `predictor variable` is independent variable used to find value of target variable\n",
    "* `Target variable` or `dependent variable` is variable whose value need to be calculated or predicted\n",
    "* `Labeled data` is the training data the model is trained on "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn\n",
    "Scikit-learn is one of the most popular machine learning libraries in Python. It's a powerful tool for building machine learning models, providing a wide range of algorithms and tools for tasks such as classification, regression, clustering, dimensionality reduction, and more. \n",
    "<br/>\n",
    "scikit-learn requires that the features are in array where each column is a feature and each row a different observation. Similarly, the target needs to be a single column with the same number of observations as the feature data\n",
    "\n",
    "`Syntax for scikit-learn`\n",
    "\n",
    "`from sklearn.module import Model` <br/>\n",
    "`model = Model()`<br/>\n",
    "`model.fit(X, y)` (X array of features, y array of our target variable values)<br/>\n",
    "`prediction = model.predict(X_new)`<br/>\n",
    "`print(prediction)`<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steps for Classifying Labels of Unseen Data\n",
    "* Build a classifier\n",
    "* Model learns from the labeled data we pass to it\n",
    "* Pass unlabeled data to the model as input\n",
    "* Model predicts the labels of the unseen data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "A supervised machine learning model which is used to predict the label of any data point by looking at `k` closest label data points. It uses majority voting which makes predictions based on what label the majority of nearest neighbors have. <br/>\n",
    "As k increases, the model becomes a simpler model. Simpler models are less able to detect relationships in the dataset, which is known as `underfitting`\n",
    "For smaller k, the model become complex and is sensitive to noise in the training data, rather than reflecting general trends, which is known as `overfitting`\n",
    "\n",
    "larger k = less complex model = can cause underfitting <br/>\n",
    "smaller k = more complex model = can lead to overfitting\n",
    "\n",
    "`from sklearn.neighbors import KNeighborsClassifier`<br/>\n",
    "`X = dataset_name[['feature1', 'feature2']]` <br/>\n",
    "`Y = dataset_name ['feature']` <br/>\n",
    "`knn = KNeighborsClassifier(n_neighbors = 6)` <br/>\n",
    "`knn.fit(X, y)` <br/>\n",
    "`predictions = knn.predict(X_new)` <br/>\n",
    "`print(predictions)`<br/>\n",
    "\n",
    "\n",
    "#### Measuring Model Performance\n",
    "In classifcation, accuracy is a commonly used metric. <br/><br/> \n",
    "`Accuracy: correct_predictions/total_observations`<br/><br/>\n",
    "For measuring model performance, it is common practice to split data into `training set` and `test set`. Training set is used to train classifier on training data and then calculate model accuracy using test set. We commonly use 20-30% of our data as the test set. Random_state argrument sets a seed for random number generator that splits the \n",
    "\n",
    "`from sklearn.model_selection import train_test_split`<br />\n",
    "`X_train, X_text, y_train, y_test = train_test_split(X, y, test_size=0.3 random_state=21)` Four arrays <br />\n",
    "`knn = KNeighborsClassifier(n_neighbors = 6)` <br/>\n",
    "`knn.fit(X_train, y_train)` <br/>\n",
    "`print(knn.score(X_test, y_test))` <br/><br/>\n",
    "\n",
    "To deal with underfitting and overfitting problem, we use incremental values of k to find value of k for which accuracy is highest\n",
    "\n",
    "`training_accuracies = {}`<br/>\n",
    "`test_accuracies = {}`<br/>\n",
    "`neighbors = np.arange(1, 26)`<br/>\n",
    "`for neighbor in neighbors:`<br/>\n",
    "`    knn = KNeighborsClassifier(n_neighbors=neighbor)`<br/>\n",
    "`knn.fit(X_train, y_train)` <br/>\n",
    "`training_accuracies[neighbor] = knn.score(X_train, y_train)` <br/>\n",
    "`test_accuracies[neighbor] = knn.score(X_test, y_test)` <br/>\n",
    "\n",
    "#### Confusion Matrix\n",
    "In machine learning, a confusion matrix is a table that is used to evaluate the performance of a classification algorithm. It allows for a detailed examination of the predicted and actual values of a model.\n",
    "\n",
    "The confusion matrix is constructed around the following concepts:\n",
    "\n",
    "* `True Positives (TP):` These are the cases where the model correctly predicts the positive class.\n",
    "* `True Negatives (TN):` These are the cases where the model correctly predicts the negative class.\n",
    "* `False Positives (FP):` These are the cases where the model incorrectly predicts the positive class when it's actually negative. Also known as Type I error.\n",
    "* `False Negatives (FN):` These are the cases where the model incorrectly predicts the negative class when it's actually positive. Also known as Type II error.\n",
    "<br/>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>Predicted Negative</th>\n",
    "        <th>Predicted Positive</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Actual Negative</td>\n",
    "        <td>True Negative (TN)</td>\n",
    "        <td>False Positive (FP)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Actual Positive</td>\n",
    "        <td>False Negative (FN)</td>\n",
    "        <td>True Positive (TP)</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br/>\n",
    "\n",
    "From the confusion matrix, various metrics can be derived to evaluate the performance of a classification model, including:\n",
    "\n",
    "* `Accuracy: (TP + TN) / (TP + TN + FP + FN)`\n",
    "    * The proportion of correctly classified instances among the total instances.\n",
    "\n",
    "* `Precision: TP / (TP + FP)`\n",
    "    * The accuracy of positive predictions, measuring the model's ability to not label a negative sample as positive.\n",
    "    * Higher precision = lower false positive rate \n",
    "\n",
    "* `Recall (Sensitivity or True Positive Rate): TP / (TP + FN)`\n",
    "    * The proportion of actual positive cases that were correctly identified, measuring the model's ability to find all the positive samples.\n",
    "    * High recall = lower false negative rate \n",
    "<br />\n",
    "\n",
    "* `F1 Score: 2 * (Precision * Recall) / (Precision + Recall)`\n",
    "    * It is the harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "\n",
    "`from sklearn.metrics import classification_report, confusion_matrix` <br/>\n",
    "`knn = KNeighborsClassifier(n_neighbors=7)` <br/>\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)` <br/>\n",
    "`knn.fit(X_train, y_train)` <br/>\n",
    "`y_pred = knn.predict(X_test)` <br/>\n",
    "`print(confusion_matrix(y_test, y_pred))` <br/>\n",
    "`print(classification_report(y_test, y_pred))` <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "`Regression` in machine learning is a type of supervised learning that deals with the prediction of continuous outcomes. It aims to find the relationship between a `dependent variable` (target) and one or more `independent variables` (features) to predict the value of the dependent variable based on the independent variables.\n",
    "\n",
    "`from sklearn.linear_model import LinearRegression` <br/>\n",
    "`reg = LinearRegression()` <br/>\n",
    "`reg.fit(X, y)` <br/>\n",
    "`predictions = reg.predict(X)` <br/>\n",
    "\n",
    "#### Regression mechanism\n",
    "In two dimension, we want to fit a line on the data and takes the form\n",
    "`y = ax + b`\n",
    "where\n",
    "* y = target\n",
    "* x = single feature\n",
    "* a, b = parameters/co-efficients of the model - slope, intercept\n",
    "\n",
    "How do we choose a and b\n",
    "* Define an error function (also called loss function) for any given line\n",
    "* Choose the line that minimizes the error function\n",
    "\n",
    "We want the line to be as close to the observation as possible. Therefore we want to minimise the vertical distance between the fit and the data. So for each observation, we calculate the vertical distance between it and the line which is called a `residual`\n",
    "\n",
    "For linear regressio in higher dimension, a line takes the form `y = a1x1 + a2x2 + a3x3 + ... + anxn + b`. \n",
    "\n",
    "So for linear regression using all features:\n",
    "\n",
    "`from sklearn.model_selection import train_test_split` <br/>\n",
    "`from sklearn.linear_model import LinearRegression`<br/>\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)`<br/>\n",
    "`reg_all = LinearRegression()`<br/>\n",
    "`reg_all.fit(X_train, y_train)`<br/>\n",
    "`y_pred = reg_all.predict(X_test)`<br/>\n",
    "\n",
    "The default metric for linear regression is `R-squared` which quantifies the amount of variance in the the target variable that is explained by the features. Its value ranges from 0 to 1 where 1 means that the features completely explain the target variance. \n",
    "\n",
    "`reg_all.score(X_test, y_test)`<br/> <br/>\n",
    "However, there is a potential pitfall; R-squared is dependent on the way the data is split where the test data mau have some peculiarities that mean the R-squared computed on it is not representative of the model's ability to generalise to unseen data\n",
    "\n",
    "The performance of linear regression can be find using `mean square error` and `root mean squared error`.\n",
    "\n",
    "`from sklearn.metrics import mean_squared_error`<br/>\n",
    "`mean_squared_error(y_test, y_pred, squared=False)`<br/>\n",
    "\n",
    "\n",
    "#### Cross Validation\n",
    "Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to estimate how well the model generalizes to new, unseen data. Here's a simpler and more concise breakdown of cross-validation in linear regression\n",
    "* ` Data Splitting:` Divide the dataset into a training set and a testing set.\n",
    "* `K-Fold Cross-Validation:` Split the training set into K subsets/folds.\n",
    "* `Iterative Training and Testing:` Train the model K times, each time using K-1 folds for training and 1 fold for testing.\n",
    "* `Performance Evaluation:` Calculate the model's performance metric (e.g., mean squared error) for each fold.\n",
    "* ` Average Metric:` Average the performance metric across all folds to determine the model's overall performance.\n",
    "* `Model Comparison and Tuning:` Compare different models or tune parameters using cross-validation results.\n",
    "* `Final Model Evaluation:` Select the best model and train it on the entire training set before evaluating it on a separate, unseen test set to estimate real-world performance.\n",
    "\n",
    "\n",
    "`from sklearn.model_selection import cross_val_score, KFold` <br/>\n",
    "`kf = KFold(n_splits=6, shuffle=True, random_state=42)`<br/>\n",
    "`reg = LinearRegression()`<br/>\n",
    "`cv_results = cross_val_score(reg, X, y, cv=kf)`<br/>\n",
    "`print(cv_results)`<br/>\n",
    "`print(np.mean(cv_results), np.std(cv_results))`<br/>\n",
    "`print(np.quantile(cv_results, [0.025, 0.975]))` (calculate 95% confidence interval)<br/>\n",
    "\n",
    "\n",
    "#### Ridge Regression\n",
    "\n",
    "Ridge regression is a technique used in linear regression to address the problem of overfitting by adding a penalty term to the ordinary least squares (OLS) method. In traditional linear regression, the goal is to minimize the residual sum of squares (RSS). However, when there is multicollinearity, the estimated coefficients can become too sensitive to the training data, leading to overfitting and high variance.\n",
    "\n",
    "`from sklearn.linear_model import Ridge` <br/>\n",
    "`scores = []`<br/>\n",
    "`for alpha in [0.1, 1.0, 10.0, 100.0, 1000.0]:`<br/>\n",
    "`ridge = Ridge(alpha=alpha)`<br/>\n",
    "`ridge.fit(X_train, y_train)`<br/>\n",
    "`y_pred = ridge.predict(X_test)`<br/>\n",
    "`scores.append(ridge.score(X_test, y_test))`<br/>\n",
    "`print(scores)`<br/>\n",
    "\n",
    "#### Lasso Regression\n",
    "Lasso regression, short for Least Absolute Shrinkage and Selection Operator, is a technique used in linear regression to perform both variable selection and regularization by adding a penalty term to the ordinary least squares (OLS) method. \n",
    "\n",
    "`from sklearn.linear_model import Lasso` <br/>\n",
    "`scores = []`<br/>\n",
    "`for alpha in [0.01, 1.0, 10.0, 20.0, 50.0]:`<br/>\n",
    "`lasso = Lasso(alpha=alpha)`<br/>\n",
    "`lasso.fit(X_train, y_train)`<br/>\n",
    "`lasso_pred = lasso.predict(X_test)`<br/>\n",
    "`scores.append(lasso.score(X_test, y_test))`<br/>\n",
    "`print(scores)`<br/><br/>\n",
    "\n",
    "* It can be used to select important features of a dataset by shrinking the coefficients of less important features to zero\n",
    "\n",
    "`from sklearn.linear_model import Lasso` <br/>\n",
    "`X = diabetes_df.drop(\"glucose\", axis=1).values`<br/>\n",
    "`y = diabetes_df[\"glucose\"].values`<br/>\n",
    "`names = diabetes_df.drop(\"glucose\", axis=1).columns`<br/>\n",
    "`lasso = Lasso(alpha=0.1)`<br/>\n",
    "`lasso_coef = lasso.fit(X, y).coef_`<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic regression is a type of statistical model used for binary classification tasks in machine learning. It predicts the probability of the occurrence of a categorical dependent variable based on one or more predictor variables. `If the probability is > 0.5. the data is labeled as 1`. It creates a linear decision boundary\n",
    "\n",
    "![Image description](logistic-regression.png)<br/><br/><br/>\n",
    "\n",
    "`from sklearn.linear_model import LogisticRegression` <br/>\n",
    "`logreg = LogisticRegression()`<br/>\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0., random_state=42)`<br/>\n",
    "`logreg.fit(X_train, y_train)`<br/>\n",
    "`y_pred = logreg.predict(X_test)`<br/>\n",
    "`print(logreg.score(X_test, y_test))`<br/>\n",
    "\n",
    "We can calculate probabilities of each instance belonging to class by calling `predict_proba` method\n",
    "\n",
    "`y_pred_probs = logreg.predict_proba(X_test)[:, 1]`<br/>\n",
    "`print(y_pred_probs[0])`\n",
    "\n",
    "##### Difference between Logistic Regression and Linear Regression:\n",
    "\n",
    "Linear regression is used for predicting continuous values by establishing a linear relationship between the dependent variable and independent variables. In contrast, logistic regression is used for binary classification, aiming to predict the probability of an input belonging to a particular class.\n",
    "\n",
    "#### ROC Curve\n",
    "The `Receiver Operating Characteristic (ROC)` curve is a graphical representation used to evaluate the performance of a classification model.\n",
    "\n",
    "`from sklearn.metrics import roc_curve` <br/>\n",
    "`fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)`<br/>\n",
    "`plt.plot([0, 1], [0, 1], 'k--)`<br/>\n",
    "`plt.plot(fpr, tpr)`<br/>\n",
    "\n",
    "To quantify the performance of the model, we calculate `Area Under Curve (AUC)`.\n",
    "\n",
    "`from sklearn.metrics import roc_auc_score` <br/>\n",
    "`print(roc_auc_score(y_test, y_pred_probs))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "In supervised machine learning, `hyperparameters` are external configurations or settings for a machine learning algorithm that are not learned from the data but are set prior to the training process. These hyperparameters control various aspects of the learning process, the model's complexity, and the optimization process. <br/><br/>\n",
    "`Hyperparameter Tuning`, also known as `Hyperparameter Optimization`, is the process of finding the best set of hyperparameters for a machine learning model to optimize its performance on a specific task.\n",
    "* We can try lots of different parameter values\n",
    "* Fit them all separately\n",
    "* See how well they perform\n",
    "* Choose the best performing values\n",
    "\n",
    "<b>Grid Search</b>: \n",
    "*   This method involves specifying a set of values or ranges for each hyperparameter, and the search algorithm     exhaustively evaluates all possible combinations. It can be computationally expensive but is straightforward.\n",
    "    * `from sklearn.model_selection import GridSearchCV` <br/>\n",
    "    `kf = KFold(n_splits=5, shuffle=True, random_state=42)`<br/>\n",
    "    `param_grid = {\"alpha\" : np.arange(0.0001, 1, 10), \"solver\":[\"sag\", \"lsqr\"]}` <br/>\n",
    "    `ridge = Ridge()`<br/>\n",
    "    `ridge_cv = GridSearchCV(ridge, param_grid, cv=kf)`<br/>\n",
    "    `ridge_cv.fit(X_train, y_train)` <br/>\n",
    "    `print(ridge_cv.best_params_, ridge_cv.best_score_)`\n",
    "\n",
    "<b>Random Search</b>:\n",
    "*  Instead of exhaustively searching all combinations, random search samples hyperparameters randomly within specified ranges. It is more computationally efficient than grid search and often yields good results.\n",
    "    * `from sklearn.model_selection import RandomizedSearchCV`<br/>\n",
    "    `kf = KFold(n_splits=5, shuffle=True, random_state=42)`<br/>\n",
    "    `param_grid = {'alpha': np.arange(*0.0001, 1, 10) 'solver': ['sag', 'lsqr]}`<br/>\n",
    "    `ridge = Ridge()`<br/>\n",
    "    `ridge_cv = RandomizedSearchCV(ridge, param_grid, cv=kf, n_iter=2)`<br/>\n",
    "    `rdige_cv.fit(X_train, y_train)`<br/>\n",
    "    `print(ridge_cv.best_params_, ridge_cv.best_score_)`<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Logistic Regression\n",
    "`Multi-class logistic regression` is an extension of binary logistic regression to handle classification problems with more than two classes. It's a type of linear model that predicts the probability of each class and assigns the class with the highest probability as the final prediction. For multi-class logistic regression, the generalization involves assigning a separate linear model to each class but combining them into a single model. <br/>\n",
    "\n",
    "In machine learning,` the one-vs-rest (OvR) strategy` is a technique for extending binary classification algorithms to handle multi-class classification problems.\n",
    "\n",
    "`lr0.fit(X, y==0)` <br/>\n",
    "`lr1.fit(X, y==1)` <br/>\n",
    "`lr2.fit(X, y==2)` <br/>\n",
    "\n",
    " In order to make predictions using one vs rest, we take the class whose classifier gives the largest raw model output or decision_function in scikit-learn terminology\n",
    "\n",
    "`lr0.decision_function(X)[0]` (6.124) <br/>\n",
    "`lr1.decision_function(X)[0]` (-5.429) <br/>\n",
    "`lr2.decision_function(X)[0]` (-7.532) <br/>\n",
    "\n",
    "In above function, classifier 0 returns highest value so it will use classifier 0 for prediction <br/>\n",
    "\n",
    "In `multinomial logistic regression`, the classifier fit a single classifier for all classes. The prediction directly outputs the best class. However, it is relatively complicated as compare to one-vs-rest strategy as their is more new code.\n",
    "\n",
    "We can use scikit-learn to fit a logistic regression model on the original mult-class data set\n",
    "\n",
    "`lr = LogisticRegression(multi_class = 'ovr')` <br/>\n",
    "`lr.fit(X, y)` <br/>\n",
    "`lr.predict(X)[0]` <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing models for different models\n",
    "Some guiding principles include\n",
    "* Size of the dataset\n",
    "    * Fewer features = simpler model, faster training time\n",
    "    * Some models like Neural Networks requires large amount of data to perform well\n",
    "* Interpretability\n",
    "    * Some models are easier to explain which can be important for stakeholders\n",
    "    * Linear Regression has higher interpretability\n",
    "* Flexibility\n",
    "    * May improve accuracy by making fewer assumptions about data\n",
    "    * KNN model is more flexible model as it doesnt assume any linear relationship between the features and the target\n",
    "\n",
    "<b>Models affected by scaling</b>\n",
    "* KNN\n",
    "* Linear Regression\n",
    "* Logistic Regression (Best Median score for performance when trained on scaled data)\n",
    "* ANN (Artificial Neural Network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print(music_df.isna().sum().sort_values())`<br/>\n",
    "Dropping missing data\n",
    "`music_df = music_df.dropna(subset=[\"genre\",\"popularity\"])` <br/>\n",
    "`print(music_df.isna().sum().sort_values())`<br/>\n",
    "Imputint values (making educated guess of what the missing value value will be. Common to use the mean, median or mode in case of categorical values )\n",
    "\n",
    "`from sklearn.impute import SimpleImputer`<br/>\n",
    "`X_cat=music_df[\"genre\"].values.reshape(-1, 1)`<br/>\n",
    "`X_num = music_df.drop([\"genre\", \"popularity\"], axis=1).values`<br/>\n",
    "`y = music_df[\"popularity\"].values`<br/>\n",
    "`X_train_cat, y_train_cat` for categorical<br/>\n",
    "`X_train_num, y_train_num` for numerical values<br/>\n",
    "`imp_cat = SimpleImputer(strategy=\"most_frequent\")` for categorical same for numerical values<br/>\n",
    "`X_train_cat = imp_cat.fit_transform(X_train_cat)`<br/>\n",
    "`X_test_cat = imp_cat.transform(X_test_cat)`<br/>\n",
    "`X_train = np.append(X_train_num, X_train_cat, axis=1)` extra step for numerical values<br/>\n",
    "\n",
    "imputiung within a pipeline\n",
    "`from sklearn.pipeline import Pipeline`<br/>\n",
    "`music_df = music_df.dropna(subset=[\"genre\", \"popularity\"])`<br/>\n",
    "`music_df[genre] =np.where(music_df[genre]== \"Rock\", 1, 0)`<br/>\n",
    "`X = music_df.drop(\"genre\", axis=1).values`<br/>\n",
    "`y = music_df[\"genre\"].values`<br/>\n",
    "`steps=[(\"imputation\", SimpleImputer()),(\"logistic_regression\", LogisticRegression())]`<br/>\n",
    "`pipeline = Pipeline(steps)`<br/>\n",
    "`X_train, X_test`<br/>\n",
    "`pipeline.fit(X_train, y_train)`<br/>\n",
    "`pipeline.score(X_test, y_test)`<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling in scikit-learn\n",
    "\n",
    "`from sklearn.preprocessing import StandardScaler`<br/>\n",
    "`scaler=StandardScaler()`<br/>\n",
    "`X_train_scaled = scaler.fit_transform(X_train)`<br/>\n",
    "`X_test_scaled = scaler.transform(X_test)`<br/>\n",
    "`print(np.mean(X), np.std(X))`<br/>\n",
    "`print(np.mean(X_trained_scaled), np.std(X_train_scaled))`<br/>\n",
    "\n",
    "Scaling in Pipeline\n",
    "\n",
    "`steps = [('scaler', StandardScaler()),('knn', KNeighborsClassifer(n_neighbors=6))]`<br/>\n",
    "`pipeline = Pipeline(steps)`<br/>\n",
    "`knn_scaled = pipeline.fit(X_train, y_train)`<br/>\n",
    "`y_pred = knn_scaled.predict(X_test)`<br/>\n",
    "`print(knn_scaled.score(X_test, y_test))`<br/>\n",
    "\n",
    "CV and scaling in pipeline\n",
    "`from sklearn.model_selection import GridSearchCV`<br/>\n",
    "`steps = [('scaler', StamdardScaler()),('knn', KNeighborsClassifier())]`<br/>\n",
    "`pipeline = Pipeline(steps)`<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Machine Learning\n",
    "It is a class of machine learning technique for discovering patterns in data. In contrast to supervised machine learning, it is learning without labels. It is purely pattern discovery unguided by prediction task\n",
    "\n",
    "`Dimension of dataset = Number of Features`\n",
    "\n",
    "### K-means Clustering\n",
    "K-means clustering is an unsupervised machine learning algorithm used for partitioning a dataset into K distinct, non-overlapping subsets (or clusters). The goal of K-means is to group similar data points into the same cluster while keeping different groups as separate as possible.\n",
    "\n",
    "`import sklearn.cluster import KMeans` <br/>\n",
    "`model = KMeans(n_clusters=3)` <br/>\n",
    "`model.fit(samples)` <br/>\n",
    "`centroid = model.cluster_centers_`\n",
    "`labels=model.predict(samples)` <br/>\n",
    "`print(labels)` <br/>\n",
    "\n",
    "New samples can be assigned to existing cluster without starting over which is done by remembering the mean of the samples in each cluster, which are called centroids. New samples  are assigned to the cluster whose centroid is closest. To visualise the result\n",
    "\n",
    "`import matplotlib.pyplot as plt` <br/>\n",
    "`xs = samples[:, 0]` <br/>\n",
    "`ys = samples[:, 2]` <br/>\n",
    "`plt.scatter(xs, ys, c=labels)` <br/>\n",
    "`plt.show()`<br/>\n",
    "\n",
    "#### Evaluating Clustering\n",
    "A good cluster has tight clusters, meaning that the samples in each cluster are bunched together, not spread out. How spread out the samples within each cluster are measued by the `inertia`. Inertia measures how far samples are from their centroids. Lower values of the interia are better.\n",
    "\n",
    "`print(model.inertia_)`\n",
    "\n",
    "The question arises `what is the best number of clusters?`. This is ultimately a trade-off; a good clustering has low inertia (tight clusters) but also doesnt have too many clusters. A good rule of thu,b is to choose an elbow in the inertia plot, a point where the inertia begins to decrease more slowly\n",
    "\n",
    "* Direct approach will be to check the correspondence with dataset feature\n",
    "* Another method is `cross-tabulation`\n",
    "\n",
    "##### Cross Tabulation\n",
    "Cross-tabulation, also known as a contingency table or cross-tab, is a statistical method used in unsupervised machine learning to analyze the relationship between two or more categorical variables. It provides a summary of the distribution of data points across these variables, allowing for a better understanding of the patterns and associations within the dataset.\n",
    "\n",
    "`import pandas as pd`<br/>\n",
    "`df = pd.DataFrame({'labels' : labels, 'species': species})`<br/>\n",
    "`print(df)`<br/>\n",
    "`ct = pd.crosstab(df['labels'], df['species'])`<br/>\n",
    "`print(ct)`<br/>\n",
    "\n",
    "How to evaluate when species (feature) information is not given\n",
    "\n",
    "##### Feature Variance\n",
    "K_means alogrithms are sensitive to feature variance. In machine learning, `feature variance` refers to the extent to which values of a particular feature in a dataset vary or spread out. High feature variance can lead to issues in model training and performance. In K-means feature variance corresponds to how much it influence the prediction. Feauters with high variance tends to have higher influence the prediction of the target variable.To give every feature a chance, the data needs to be transformed so that features have equal variance.\n",
    "\n",
    "`from sklearn.preprocessing import StandardScaler`<br/>\n",
    "`scaler = StandardScaler()`<br/>\n",
    "`scaler.fit(samples)`<br/>\n",
    "`StandardScaler(copy=True, with_mean=True, with_std=True)`<br/>\n",
    "`samples_scaled=scaler.transform(samples)`<br/>\n",
    "\n",
    "Pipeline\n",
    "\n",
    "`from sklearn.preprocessing import StandardScaler`<br/>\n",
    "`from sklearn.cluster import KMeans`<br/>\n",
    "`scaler = StandardScaler()`<br/>\n",
    "`kmeans = KMeans(n_clusters=3)`<br/>\n",
    "`from sklearn.pipleine import make_pipeline`<br/>\n",
    "`pipeline = make_pipeline(scaler, kmeans)`<br/>\n",
    "`pipeline.fit(samples)`<br/>\n",
    "`labels = pipeline.predict(samples)`<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Hierarchies\n",
    "`Hierarchical clustering` is a method used in unsupervised machine learning for grouping similar data points into clusters. The process is hierarchical in nature because it creates a tree-like structure (dendrogram) of clusters. In such clustering, the clusters are contained in one another. The dendrogram groups the countries into larger and larger clusters. Dendograms are read from the bottom up where the vertical lines represent clusters. The y-axis of the dendrogram encodes the distance between merging clusters. The distance between two clusters is measured using a `linkage method` which in the following example is `complete`. In `complete linkage`, the distance between clusters is the distance between the furthest points of the clusters. In `single linkage`, the distance between clusters is the distance between the closest point of the clusters<br/>\n",
    "Hierarchical clustering in following steps (based on Eurovision dataset):\n",
    "* Every country begins in a separate cluster\n",
    "* At each step, the two closest clusters are merged\n",
    "* Continue untill all countries in a single cluster\n",
    "This process is particular type of hierarchical clustering called `agglomerative clustering`\n",
    "\n",
    "`import matplotlib.pyplot as plt` <br/>\n",
    "`from scipy.cluster.hierarchy import linkage, dendrogram`<br/>\n",
    "`mergings = linkage(samples, method=complete)` performs the hierarchical clustering<br/>\n",
    "`dendrogram(mergings, labels=country_names, leaf_rotation=90, leaf_font_size=6)`<br/>\n",
    "`plt.show()`\n",
    "\n",
    "#### Extracting cluster labels using fcluster\n",
    "`from scipy.cluster.hierarchy import linkage`<br/>\n",
    "`mergings = linkage(samples, method='complete')`<br/>\n",
    "`from scipy.cluster.hierarchy import fcluster`<br/>\n",
    "`labels = fcluster(mergings, 15, criterion='distance')`<br/>\n",
    "`print(labels)`\n",
    "\n",
    "To inspect cluster labels\n",
    "\n",
    "`import pandas as pd`<br/>\n",
    "`pairs = pd.DataFrame({'labels': labels, 'countries': country_names})`<br/>\n",
    "`print(pairs.sort_values('labels'))`<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "t-SNE, or `t-Distributed Stochastic Neighbor Embedding`, is a dimensionality reduction technique commonly used in machine learning and data visualization. Its primary purpose is to take high-dimensional data and represent it in a lower-dimensional space, typically 2D or 3D, while preserving the pairwise similarities between data points as much as possible. In simple terms, it maps samples from their high-dimensional space into a 2- or 3-dimensional space so they can be visualised. It perfroms excellent job of approximately representing the distances between the samples\n",
    "\n",
    "##### t-SNE on the iris dataset\n",
    "`import mtplotlib.pyplot as plt` <br/>\n",
    "`from sklearn.mainfold import TSNE` <br/>\n",
    "`model = TSNE(learning_rate=100)` <br/>\n",
    "`transformed = model.fit_transform(samples)` <br/>\n",
    "`xs = transformed[:, 0]` <br/>\n",
    "`ys = transformed[:, 1]` <br/>\n",
    "`plt.scatter(xs, ys, c=species)` <br/>\n",
    "`plt.show()` <br/>\n",
    "\n",
    "t-SNE has only fit_transform() method that simultaneously fits the model and transforms the data. It doesnt have separate `fit()` or `tranform()` methods which means that you cant extend a t-SNE map to include new samples; instead, you have start over each time <br/>\n",
    "Choosing learning rate for t-SNE is a bit complicated. If points on the scatter plot are bunhed together, you have made a wrong choice. Normally, its enough to try a few values between 50 and 200. <br/>\n",
    "Another thing to be aware of is that the axes of a t-SNE plot do not have any interpretable meaning. In fact, they are different every time t-SNE is applied even on the same data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction\n",
    "Dimension reduction in machine learning is a technique used to reduce the number of features or variables in a dataset. The goal is to simplify the dataset while retaining its essential information. High-dimensional data, where each instance has many features, can be challenging to work with due to increased computational complexity, potential noise, and the curse of dimensionality. It is called principal component analysis because it learns the `principal components` of the data\n",
    "\n",
    "### Principal Component Analysis\n",
    "`Principal Component Analysis (PCA)` is a dimensionality reduction technique widely used in machine learning and statistics. Its main goal is to transform high-dimensional data into a lower-dimensional representation, preserving as much of the original variance as possible. <br/>\n",
    "Performs PCA in two steps\n",
    "* Decorrelation\n",
    "    * PCA rotates the data samples so that they are aligned with the coordinate axes.\n",
    "    * It also shifts the sample so that they have mean zero\n",
    "* Reducing Dimension\n",
    "\n",
    "`from sklearn.decomposition import PCA` <br/>\n",
    "`model = PCA()` <br/>\n",
    "`model.fit(samples)` <br/>\n",
    "`transformed = model.transform(samples)` <br/>\n",
    "`print(transformed)` <br/>\n",
    "`print(model.components_)` <br/>\n",
    " \n",
    "The new array has the same number of rows and columns as the original sample array. The columns of new array correspond to PCA features\n",
    "\n",
    "##### Pearson Correlation\n",
    "Pearson correlation, often referred to as the `Pearson correlation coefficient (PCC)` or Pearson's r, is a statistical measure that quantifies the linear relationship between two variables. It is commonly used to assess the strength and the direction of the linear association between two continuous variables. Value varies between -1 and  1 where 0 means no linear correlation\n",
    "\n",
    "##### Intrinsic Dimension\n",
    "`Intrinsic dimension` in machine learning refers to the essential number of features needed to represent the data accurately. It is the dimensionality of the dataset that captures most of the variability or information within the data. In other words, intrinsic dimensionality helps identify the minimum number of features that retain the key information necessary to describe the dataset.\n",
    "\n",
    "`Intrinsic Dimension using Iris dataset`\n",
    "To better illustrate the intrinsic dimension, lets consider an example dataset containing only some of the samples from the iris dataset. We will take three measurments (3 features) from the iris versicolor sample:\n",
    "* sepal length\n",
    "* sepal width\n",
    "* petal width\n",
    "\n",
    "Each sample point is represented as a point in 3d space. If we make a 3D scatter plot of the samples, you can observe they all lie close to a flat 2D sheet, which means the data can be approximated by using only two coordinates without losing much information. So this dataset has intrinsic dimension 2. The intrinsic dimension can be identified by counting the PCA features that have high variance\n",
    "\n",
    "`import matplotlib.pyplot as plt` <br/>\n",
    "`from sklearn.decomposition import PCA` <br/>\n",
    "`pca = PCA()` <br/>\n",
    "`pca.fit(samples)` <br/>\n",
    "`features = range(pca.n_components)` <br/>\n",
    "`plt.bar(features, pca.explained_variance)`<br/>\n",
    "`plt.xticks(features)` <br/>\n",
    "`plt.show()` <br/>\n",
    "\n",
    "Intrinsic dimension is an idealization but there is not always one correct answer. <br.><br/>\n",
    "PCA discards low variance PCA features and assumes the high variance features are informative. For dimension reduction\n",
    "\n",
    "`from sklearn.decomposition import PCA` <br/>\n",
    "`pca = PCA(n_components=2)`<br/>\n",
    "`pca.fit(samples)` <br/>\n",
    "`transformed=pca.transform(samples)`<br/>\n",
    "`print(transformed.shape)`<br/>\n",
    "\n",
    "### Non Negative Matrix Factorization\n",
    "`Non-Negative Matrix Factorization (NMF)` is a technique used in machine learning for dimensionality reduction and feature extraction. It's particularly useful when dealing with non-negative data, such as images, text, audio, or other types of non-negative signals. Unlike PCA, NPF are easier to understand and much easier to explain to others. For example, NMF expresses images as combination of patterns. It requires the dataset to have only non-negative sample features\n",
    "\n",
    "`Word Frequency Example`<br/>\n",
    "The frequency of words in each document can be calculated using `tfidf`. `tf` is the frequency of the word in the document while `idf` is a weighting scheme that reduces the influence of words\n",
    "\n",
    "`from sklearn.decomposition import NMF`<br/>\n",
    "`model = NMF(n-components=2)`<br/>\n",
    "`model.fit(samples)`<br/>\n",
    "`nmf_features = model.transform(samples)`<br/>\n",
    "`print(model.components_)`<br/>\n",
    "`print(nmf_features)` <br/>\n",
    "\n",
    "Sample can be reconstructed by multiplying the NMF components by NMF feature values and adding up. This calculation can also be expressed as product of matrics\n",
    "\n",
    "`Example`\n",
    "\n",
    "NMF Component value is\n",
    "<br/>\n",
    "[[1.  0.5 0. ]\n",
    " [0.2 0.1 2.1]]\n",
    "<br/>\n",
    "\n",
    "NMF Feature value is [2, 1]\n",
    "<br/>\n",
    "\n",
    "The reconstruction formula is \n",
    "\n",
    "`Reconstructed Sample = NMF Feature Values x NMF Components`\n",
    "\n",
    "Performing the multiplication\n",
    "\n",
    "[2, 1] x [[1.  0.5 0. ]\n",
    " [0.2 0.1 2.1]] \n",
    " <br/>\n",
    " = [ 2(1) + 1(0.2) + 2(0.5) + 1(0.1) + 2(0)+1(2.1) ]\n",
    " <br/>\n",
    " = [2.2, 1.1, 2.1]\n",
    " <br/> <br/> <br/>\n",
    "* If NMF is applied to documents, then the NMF components correspond to and NMF Features reconstruct the documents from the topic\n",
    "* If NMF is applied to images, then the NMF components represent patterns that frequently occur in the image\n",
    "* A collection of grayscale images of the same size can be encoded as 2D array in which each row corresponds to an image as flattedned array and each column represents a pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How linear classifiers make predictions\n",
    "\n",
    "* First we compute raw model output \n",
    "    * raw model output = coefficients.features + intercept\n",
    "\n",
    "    `lr = LogisticRegression()` <br/>\n",
    "    `lr.fit(X, y)` <br/>\n",
    "    `lr.predict(X)[10]` <br/>\n",
    "    `lr.predict(X)[20]` <br/>\n",
    "    `lr.coef_ @X[10] + lr.intercept` fives raw model output\n",
    "\n",
    "* We will then take the sign of this quantity (check wether it is positive or negative, and same for both SVMs and Regression)\n",
    "    * if positive, predict `1` class\n",
    "    * If negative, predict `0` class\n",
    "\n",
    "    \n",
    "In general, this is what the predict function does for any X; it computes the raw model output. checks if its positive or negative and then returns a result based on the names of the classes in your data set, 0 and 1\n",
    "\n",
    "`Both logistic regression and linear SVM has different fit functions but same predict function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "`Support Vector Machines (SVM)` is a supervised machine learning algorithm that is used for classification and regression tasks. It is particularly effective in high-dimensional spaces and is widely used for tasks like image classification, text classification, and handwriting recognition. They use `hinge loss` function and L2 regularisation. The SVM maximizes the margin (distnace from the boundary to the closest points) for linearly separable datasets.\n",
    "<br/>\n",
    "In scikit-learn, the basic SVM classifier is called LinearSVC which works the same way as logistic regression\n",
    "\n",
    "`from sklearn.svm import LinearSVC` <br/>\n",
    "`svm = LinearSVC()`<br/>\n",
    "`svm.fit(feature, target)`<br/>\n",
    "`svm.score(feature, target)`<br/>\n",
    "\n",
    "### Support Vector Classification\n",
    "`Support Vector Classification (SVC)` is a specific implementation of SVM designed for classification tasks. The goal of SVC is to find a hyperplane that separates the data into different classes while maximizing the margin between these classes\n",
    "\n",
    "`from sklearn.svm import SVC`<br/>\n",
    "`svm = SVC()`<br/>\n",
    "`svm.fit(data, target)`<br/>\n",
    "`svm.score(data, target)`<br/>\n",
    "\n",
    "\n",
    "#### Kernel SVMs\n",
    " Kernel SVMs, or `Kernelized Support Vector Machines`, are an extension of traditional SVMs that allow for the application of non-linear decision boundaries through the use of kernels. Kernels in SVM allow the algorithm to implicitly map the input features into a higher-dimensional space, where it becomes easier to find a hyperplane that separates the data. The mapping is done without explicitly calculating the coordinates of the data points in the higher-dimensional space\n",
    "\n",
    " `from sklearn.svm import SVC` <br/>\n",
    " `svm = SVC(gamma=1)` (default is kernel = 'rbf' (Radial Basis Function) ) (gamma controls the smoothenss of the boundary) <br/>\n",
    "\n",
    " It is not advisible to use largest value of gamma and get the highest possible training accuracy beacuse it leads to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "A `loss function`, also known as a cost function or objective function, is a mathematical function that quantifies the difference between the predicted values and the actual values (or labels) in a dataset. The loss function serves as a measure of how well the model is performing. It provides a way to calculate the error between the predicted output and the actual target values. Example `Mean Squared Error (MSE)`. The square error is not appropriate for classication problems. A natural loss for classificaiton problem is the number of errors. This is the  `0-1 loss`; its 0 for a correct prediction and 1 for an incorrect prediction. By summing, we get the number of errors made on training dataset. However, in reality it is difficult to do and that is why its not used my logistic regression and svm\n",
    "<br/>\n",
    "For minimizing a loss\n",
    "\n",
    "`from scipy.optimize import minimize`\n",
    "`minimize(np.square,0).x`\n",
    "``\n",
    "\n",
    "<br/>\n",
    "\n",
    "We can think of `fit()` function as running code that minimizes the loss. The `score()` method is used to how well the model is doing on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Logistic Regression</th>\n",
    "    <th>SVM</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Is a linear model</td>\n",
    "    <td>Is a linear model</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Can use with kernels but slow</td>\n",
    "    <td>Can use with kernels and fast</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Outputs meaningful probabilities</td>\n",
    "    <td>Does not naturally output probabilities</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Sensitive to outliers as it aims to maximize the likelihood of the observed data</td>\n",
    "    <td>Relatively robut to outliers due to the focus on maximizing the margin</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Can be extended to multiclass</td>\n",
    "    <td>Can be extended to multiclass</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Generally faster and less computationally intensive, especially for large datasets.</td>\n",
    "    <td>Can be computationally demanding, particularly with large datasets, as it involves solving a quadratic programming problem.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>All data points affect fit</td>\n",
    "    <td>Only `support vectors` affect fit</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>L2 or L1 Regularisation</td>\n",
    "    <td>Conentionally just L2</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "A `decision tree` is a popular supervised machine learning algorithm used for both classification and regression tasks. It works by recursively partitioning the input space into regions and assigning a specific label or value to each region. Three is trained in such a way so that in each leaf one class-label is predominant\n",
    "\n",
    "* `Nodes`: <br/>\n",
    "Decision trees consist of nodes, which represent questions or conditions based on features. Three kind of node\n",
    "    * `Root node` is the node at which the decision-tree starts growing. It has no parent group and involves a question that gives rise to 2 children node through branches\n",
    "    * `Internal node` has one parent node and involves a question that gives rise to 2 children nodes\n",
    "    * `Leaf node` is a node with one parent node and no children. It is where predictions are made\n",
    "* `Edges`: <br/>\n",
    "Edges connect nodes and represent possible answers to the questions.\n",
    "* `Leaves`: <br/>\n",
    "Terminal nodes, or leaves, contain the final output (class label for classification or predicted value for regression).\n",
    "\n",
    "\n",
    "`from sklearn.tree import DecisionTreeClassifier` <br/>\n",
    "`import sklearn.model_selection import train_test_split` <br/>\n",
    "`from sklearn.metrics import accuracy_score` <br/>\n",
    "`X_train, X_test, y_train, y_test` <br/>\n",
    "`dt = DecisionTreeClassifier(max_depth=2, random_state=1, criterion='gini')` <br/>\n",
    "`dt.fit(X_train, y_train)` <br/>\n",
    "`y_pred = dt.predict(X_test)` <br/>\n",
    "`accuracy_score(y_test, y_pred)` <br/>\n",
    "\n",
    "`Decision region` is the region in the feature space where all instances are assigned to one class label. Decision regions are separated by surfaces called `decision boundaries`\n",
    "\n",
    "### CART\n",
    "CART, which stands for `Classification and Regression Trees`, is a popular machine learning algorithm used for both classification and regression tasks. The process includes\n",
    "\n",
    "* `Recursive Splitting:` <br/>\n",
    "Start with the entire dataset and recursively split nodes based on features to maximize class separation (Gini impurity for classification, mean squared error for regression).\n",
    "* `Binary Tree Structure: ` <br/>\n",
    "Each internal node tests a feature, and the branches represent possible outcomes. Leaves contain class labels (for classification) or predicted values (for regression). \n",
    "* `Stopping Criteria: ` <br/>\n",
    "Define criteria to stop splitting, such as a maximum tree depth, minimum samples per leaf, or minimum impurity improvement.\n",
    "* `Pruning (Optional): ` <br/>\n",
    "After tree construction, prune nodes to improve generalization and reduce overfitting.\n",
    "* `Prediction: ` <br/>\n",
    "For a new instance, traverse the tree, and the prediction is the majority class (for classification) or mean/median (for regression) in the leaf node. <br/> <br/>\n",
    "\n",
    "Advantages of CARTs include:\n",
    "* Simple to understand\n",
    "* Simple to interpret\n",
    "* Easy to use\n",
    "* Flexible, which gives them an ability to describte non-linear dependencies between features and labels\n",
    "* Dont need a lot of feature preprocessing to train\n",
    "\n",
    "Limitation of CARTs include:\n",
    "* A classification tree is only able to produce orthogonal decision boundaries\n",
    "* Sensitive to small variations in the training dataset\n",
    "* Also suffer from high variance when they are trained without constraints\n",
    "\n",
    "To use Decsion Tree for Regression problems\n",
    "\n",
    "`from sklearn.tree import DecisionTreeRegressor` <br/>\n",
    "`from sklearn.metrics import mean_squared_error as MSE` <br/>\n",
    "`dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.1, random_state=3)` <br/>\n",
    "`dt.fit(X_train, y_train)` <br/>\n",
    "`y_pred=dt.predict(X_test)` <br/>\n",
    "`mse_dt = MSE(y_test, y_pred)` <br/>\n",
    "`rmse_dt = mse_dt**(1/2)` <br/>\n",
    "`print(rmse_dt)` <br/>\n",
    "\n",
    "\n",
    "#### Information Gain\n",
    "`Information Gain` is a concept used to measure the effectiveness of a feature in classifying the data. It helps decide which feature to use for splitting nodes in the tree during the construction process.  Information Gain measures the reduction in entropy or impurity achieved by splitting a dataset based on a particular feature. The decision tree choose a feature and split-point by maximizing information gain. The tree considers that every node contains information and aims at maximizing the information gain obtained after each split\n",
    "\n",
    "#### KFold CV\n",
    "`K-Fold cross-validation (CV)` is a technique used to assess the model's performance and generalization ability. It involves dividing the dataset into k subsets, called folds, and iteratively training and evaluating the model k times, using a different fold as the test set in each iteration. <br/>\n",
    "* If CV error of the model function is greater than training set error of model function, then the model function suffers from high variance\n",
    "    * It means the model is overfitting the training dataset. To remedy overfitting,\n",
    "        * decrease model complexity\n",
    "        * Increase data size (gather more data)\n",
    "\n",
    "* If CV error of the model is roughly equal to the training error but much greater than the desired error, then the model suffers from high bias\n",
    "    * Such model is said to underfitting the training set. To remedy underfitting,\n",
    "        * increase model complexity\n",
    "        * icrease max_depth, decrease min samples per leaf\n",
    "        * gather more relevant features\n",
    "\n",
    "`from sklearn.model_selection import cross_val_score` <br/>\n",
    "`X_train, X_test ...` <br/>\n",
    "`dt = DecisionTreeRegressor(max_depth= , min_samples_leaf= , random_state= )` <br/>\n",
    "`MSE_CV = -cross_val_score(dt, X_train, y_train, cv= , scoring='neg_mean_squared_error', n_jobs = -1)`<br/>\n",
    "`dt.fit(X_train, y_train)` <br/>\n",
    "`y_predict_train = dt.predict(X_train)` <br/>\n",
    "`y_predict_test = dt.predict(X_test)`<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "`Ensemble learning` is a machine learning paradigm that involves combining the predictions of multiple models to produce a more robust and accurate prediction than any individual model.\n",
    "* Different models are trained on the same dataset\n",
    "* Each model make its own predictions\n",
    "* Meta-model then aggregates the predictions of individual models and outputs a final prediction\n",
    "* Final prediction is more robust and less prone to errors than each individual model\n",
    "\n",
    "### Voting Classifier\n",
    "A `voting classifier` is an ensemble learning technique in machine learning where multiple models are trained to make predictions on a dataset, and the final prediction is determined by combining the individual predictions through a \"voting\" mechanism. The meta model outputs the prediction through hard voting. In `hard voting`, each model in the ensemble \"votes\" for a class, and the class that receives the majority of the votes is selected as the final prediction.\n",
    "This is suitable for classifiers that output discrete class labels.\n",
    "\n",
    "`from sklearn.linear_model import LogisticRegression` <br/>\n",
    "`from sklearn.tree import DecisionTreeClassifier` <br/>\n",
    "`from sklearn.neighbors import KNeighborsClassifier as KNN` <br/>\n",
    "`from sklaern.ensemble import VotingClassifier` <br/>\n",
    "`X_train, X_test, y_train, y_test` <br/>\n",
    "`lr = LogisticRegression(random_state=)` <br/>\n",
    "`knn = KNN()` <br/>\n",
    "`dt = DecisionTreeClassifier(random_state=)` <br/>\n",
    "`classifiers=[('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]` <br/>\n",
    "`for clf_name in classifiers: ` <br/>\n",
    "    `clf.fit(X_train, y_-_train)`<br/>\n",
    "`vc = VotingClassifier(estimators=classifiers)` <br/>\n",
    "`vc.fit(X_train, y_train)` <br/>\n",
    "`y_pred = vc.predict(X_test)` <br/>\n",
    "\n",
    "#### Bagging\n",
    "`Bagging`, or `Bootstrap Aggregating`, is a machine learning ensemble technique designed to improve the stability and accuracy of a predictive model. The basic idea behind bagging is to train multiple instances of the same learning algorithm on different subsets of the training data and then combine their predictions. Bagging helps in reducing overfitting and variance in the model. By training multiple models on different subsets of the data, the ensemble model becomes more robust and less sensitive to the peculiarities of the training set. <br/>\n",
    "The` bootstrap method` is a statistical tool in machine learning that helps estimate the uncertainty of a statistic. It works by repeatedly drawing random samples (with replacement) from your data. Each sample is used to calculate the statistic of interest. The variability in these estimates gives insights into the uncertainty associated with your initial data. <br/>\n",
    "In classification problem, the final prediction is obtained by majority voting. In regression problem, the final prediction is the average of the predictions made by the individual models forming the ensemble\n",
    "\n",
    "`from sklearn.ensemble import BaggingClassifer` <br/>\n",
    "`from sklearn.tree import DecisionTreeClassifier` <br/>\n",
    "`X_train, X_test, ....` <br/>\n",
    "`dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16)` <br/>\n",
    "`bc = BaggingClassifier(base_estimator=dt, n_estimators=300m, n_jobs=-1)` <br/>\n",
    "`bc.fit(X_train, y_train)` <br/>\n",
    "`y_pred = bc.predict(X_test)` <br/>\n",
    "`accuracy = accuracy_score(y_test, y_pred)` <br/>\n",
    "\n",
    "On average, for each model, 63$ of the training instances are sampled. The remaining 37% are not sampled constitute what is known as the `Out-of-Bag` or `OOB instances`. Since OOB instances are not seen by a model during training, these can be used to estimate the perfromance of the ensemble without the need for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
