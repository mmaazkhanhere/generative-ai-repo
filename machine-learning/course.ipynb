{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Machine learning is the process whereby machines are given the ability to learn to make decisions from data without being explicity programmed.\n",
    "* <b>`Unsupervised Learning`</b> is the process of uncovering hidden patterns and structures from unlabeled data.\n",
    "    * Example grouping customers into distinct categories (`known as clustering`) based on various criteria like shopping behavior, most bought products etc \n",
    "* <b>`Supervised Learning`</b> is type of machine learning where the values to be predicted are already known and model is build to predict target values of unseen given data, given the features\n",
    "    * Uses features to predict the value of target variable\n",
    "\n",
    "## Types of Supervised Learning\n",
    "* <b>`Classification`</b> is used to predict the label, or cateogry of an observation. The target variable consists of categories\n",
    "    * Example cat or dog, spam email classification\n",
    "* <b>`Regression`</b> is used to predict continuous values\n",
    "    * Example property price prediction, stock forecasting\n",
    "\n",
    "### Naming Convention\n",
    "* `Feature` or `predictor variable` is independent variable used to find value of target variable\n",
    "* `Target variable` or `dependent variable` is variable whose value need to be calculated or predicted\n",
    "* `Labeled data` is the training data the model is trained on "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn\n",
    "Scikit-learn is one of the most popular machine learning libraries in Python. It's a powerful tool for building machine learning models, providing a wide range of algorithms and tools for tasks such as classification, regression, clustering, dimensionality reduction, and more. \n",
    "<br/>\n",
    "scikit-learn requires that the features are in array where each column is a feature and each row a different observation. Similarly, the target needs to be a single column with the same number of observations as the feature data\n",
    "\n",
    "`Syntax for scikit-learn`\n",
    "\n",
    "`from sklearn.module import Model` <br/>\n",
    "`model = Model()`<br/>\n",
    "`model.fit(X, y)` (X array of features, y array of our target variable values)<br/>\n",
    "`prediction = model.predict(X_new)`<br/>\n",
    "`print(prediction)`<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steps for Classifying Labels of Unseen Data\n",
    "* Build a classifier\n",
    "* Model learns from the labeled data we pass to it\n",
    "* Pass unlabeled data to the model as input\n",
    "* Model predicts the labels of the unseen data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "A supervised machine learning model which is used to predict the label of any data point by looking at `k` closest label data points. It uses majority voting which makes predictions based on what label the majority of nearest neighbors have. <br/>\n",
    "As k increases, the model becomes a simpler model. Simpler models are less able to detect relationships in the dataset, which is known as `underfitting`\n",
    "For smaller k, the model become complex and is sensitive to noise in the training data, rather than reflecting general trends, which is known as `overfitting`\n",
    "\n",
    "larger k = less complex model = can cause underfitting <br/>\n",
    "smaller k = more complex model = can lead to overfitting\n",
    "\n",
    "`from sklearn.neighbors import KNeighborsClassifier`<br/>\n",
    "`X = dataset_name[['feature1', 'feature2']]` <br/>\n",
    "`Y = dataset_name ['feature']` <br/>\n",
    "`knn = KNeighborsClassifier(n_neighbors = 6)` <br/>\n",
    "`knn.fit(X, y)` <br/>\n",
    "`predictions = knn.predict(X_new)` <br/>\n",
    "`print(predictions)`<br/>\n",
    "\n",
    "\n",
    "#### Measuring Model Performance\n",
    "In classifcation, accuracy is a commonly used metric. <br/><br/> \n",
    "`Accuracy: correct_predictions/total_observations`<br/><br/>\n",
    "For measuring model performance, it is common practice to split data into `training set` and `test set`. Training set is used to train classifier on training data and then calculate model accuracy using test set. We commonly use 20-30% of our data as the test set. Random_state argrument sets a seed for random number generator that splits the \n",
    "\n",
    "`from sklearn.model_selection import train_test_split`<br />\n",
    "`X_train, X_text, y_train, y_test = train_test_split(X, y, test_size=0.3 random_state=21)` Four arrays <br />\n",
    "`knn = KNeighborsClassifier(n_neighbors = 6)` <br/>\n",
    "`knn.fit(X_train, y_train)` <br/>\n",
    "`print(knn.score(X_test, y_test))` <br/><br/>\n",
    "\n",
    "To deal with underfitting and overfitting problem, we use incremental values of k to find value of k for which accuracy is highest\n",
    "\n",
    "`training_accuracies = {}`<br/>\n",
    "`test_accuracies = {}`<br/>\n",
    "`neighbors = np.arange(1, 26)`<br/>\n",
    "`for neighbor in neighbors:`<br/>\n",
    "`    knn = KNeighborsClassifier(n_neighbors=neighbor)`<br/>\n",
    "`knn.fit(X_train, y_train)` <br/>\n",
    "`training_accuracies[neighbor] = knn.score(X_train, y_train)` <br/>\n",
    "`test_accuracies[neighbor] = knn.score(X_test, y_test)` <br/>\n",
    "\n",
    "#### Confusion Matrix\n",
    "In machine learning, a confusion matrix is a table that is used to evaluate the performance of a classification algorithm. It allows for a detailed examination of the predicted and actual values of a model.\n",
    "\n",
    "The confusion matrix is constructed around the following concepts:\n",
    "\n",
    "* `True Positives (TP):` These are the cases where the model correctly predicts the positive class.\n",
    "* `True Negatives (TN):` These are the cases where the model correctly predicts the negative class.\n",
    "* `False Positives (FP):` These are the cases where the model incorrectly predicts the positive class when it's actually negative. Also known as Type I error.\n",
    "* `False Negatives (FN):` These are the cases where the model incorrectly predicts the negative class when it's actually positive. Also known as Type II error.\n",
    "<br/>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>Predicted Negative</th>\n",
    "        <th>Predicted Positive</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Actual Negative</td>\n",
    "        <td>True Negative (TN)</td>\n",
    "        <td>False Positive (FP)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Actual Positive</td>\n",
    "        <td>False Negative (FN)</td>\n",
    "        <td>True Positive (TP)</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br/>\n",
    "\n",
    "From the confusion matrix, various metrics can be derived to evaluate the performance of a classification model, including:\n",
    "\n",
    "* `Accuracy: (TP + TN) / (TP + TN + FP + FN)`\n",
    "    * The proportion of correctly classified instances among the total instances.\n",
    "\n",
    "* `Precision: TP / (TP + FP)`\n",
    "    * The accuracy of positive predictions, measuring the model's ability to not label a negative sample as positive.\n",
    "    * Higher precision = lower false positive rate \n",
    "\n",
    "* `Recall (Sensitivity or True Positive Rate): TP / (TP + FN)`\n",
    "    * The proportion of actual positive cases that were correctly identified, measuring the model's ability to find all the positive samples.\n",
    "    * High recall = lower false negative rate \n",
    "<br />\n",
    "\n",
    "* `F1 Score: 2 * (Precision * Recall) / (Precision + Recall)`\n",
    "    * It is the harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "\n",
    "`from sklearn.metrics import classification_report, confusion_matrix` <br/>\n",
    "`knn = KNeighborsClassifier(n_neighbors=7)` <br/>\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)` <br/>\n",
    "`knn.fit(X_train, y_train)` <br/>\n",
    "`y_pred = knn.predict(X_test)` <br/>\n",
    "`print(confusion_matrix(y_test, y_pred))` <br/>\n",
    "`print(classification_report(y_test, y_pred))` <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "`Regression` in machine learning is a type of supervised learning that deals with the prediction of continuous outcomes. It aims to find the relationship between a `dependent variable` (target) and one or more `independent variables` (features) to predict the value of the dependent variable based on the independent variables.\n",
    "\n",
    "`from sklearn.linear_model import LinearRegression` <br/>\n",
    "`reg = LinearRegression()` <br/>\n",
    "`reg.fit(X, y)` <br/>\n",
    "`predictions = reg.predict(X)` <br/>\n",
    "\n",
    "#### Regression mechanism\n",
    "In two dimension, we want to fit a line on the data and takes the form\n",
    "`y = ax + b`\n",
    "where\n",
    "* y = target\n",
    "* x = single feature\n",
    "* a, b = parameters/co-efficients of the model - slope, intercept\n",
    "\n",
    "How do we choose a and b\n",
    "* Define an error function (also called loss function) for any given line\n",
    "* Choose the line that minimizes the error function\n",
    "\n",
    "We want the line to be as close to the observation as possible. Therefore we want to minimise the vertical distance between the fit and the data. So for each observation, we calculate the vertical distance between it and the line which is called a `residual`\n",
    "\n",
    "For linear regressio in higher dimension, a line takes the form `y = a1x1 + a2x2 + a3x3 + ... + anxn + b`. \n",
    "\n",
    "So for linear regression using all features:\n",
    "\n",
    "`from sklearn.model_selection import train_test_split` <br/>\n",
    "`from sklearn.linear_model import LinearRegression`<br/>\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)`<br/>\n",
    "`reg_all = LinearRegression()`<br/>\n",
    "`reg_all.fit(X_train, y_train)`<br/>\n",
    "`y_pred = reg_all.predict(X_test)`<br/>\n",
    "\n",
    "The default metric for linear regression is `R-squared` which quantifies the amount of variance in the the target variable that is explained by the features. Its value ranges from 0 to 1 where 1 means that the features completely explain the target variance. \n",
    "\n",
    "`reg_all.score(X_test, y_test)`<br/> <br/>\n",
    "However, there is a potential pitfall; R-squared is dependent on the way the data is split where the test data mau have some peculiarities that mean the R-squared computed on it is not representative of the model's ability to generalise to unseen data\n",
    "\n",
    "The performance of linear regression can be find using `mean square error` and `root mean squared error`.\n",
    "\n",
    "`from sklearn.metrics import mean_squared_error`<br/>\n",
    "`mean_squared_error(y_test, y_pred, squared=False)`<br/>\n",
    "\n",
    "\n",
    "#### Cross Validation\n",
    "Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to estimate how well the model generalizes to new, unseen data. Here's a simpler and more concise breakdown of cross-validation in linear regression\n",
    "* ` Data Splitting:` Divide the dataset into a training set and a testing set.\n",
    "* `K-Fold Cross-Validation:` Split the training set into K subsets/folds.\n",
    "* `Iterative Training and Testing:` Train the model K times, each time using K-1 folds for training and 1 fold for testing.\n",
    "* `Performance Evaluation:` Calculate the model's performance metric (e.g., mean squared error) for each fold.\n",
    "* ` Average Metric:` Average the performance metric across all folds to determine the model's overall performance.\n",
    "* `Model Comparison and Tuning:` Compare different models or tune parameters using cross-validation results.\n",
    "* `Final Model Evaluation:` Select the best model and train it on the entire training set before evaluating it on a separate, unseen test set to estimate real-world performance.\n",
    "\n",
    "\n",
    "`from sklearn.model_selection import cross_val_score, KFold` <br/>\n",
    "`kf = KFold(n_splits=6, shuffle=True, random_state=42)`<br/>\n",
    "`reg = LinearRegression()`<br/>\n",
    "`cv_results = cross_val_score(reg, X, y, cv=kf)`<br/>\n",
    "`print(cv_results)`<br/>\n",
    "`print(np.mean(cv_results), np.std(cv_results))`<br/>\n",
    "`print(np.quantile(cv_results, [0.025, 0.975]))` (calculate 95% confidence interval)<br/>\n",
    "\n",
    "\n",
    "#### Ridge Regression\n",
    "\n",
    "Ridge regression is a technique used in linear regression to address the problem of overfitting by adding a penalty term to the ordinary least squares (OLS) method. In traditional linear regression, the goal is to minimize the residual sum of squares (RSS). However, when there is multicollinearity, the estimated coefficients can become too sensitive to the training data, leading to overfitting and high variance.\n",
    "\n",
    "`from sklearn.linear_model import Ridge` <br/>\n",
    "`scores = []`<br/>\n",
    "`for alpha in [0.1, 1.0, 10.0, 100.0, 1000.0]:`<br/>\n",
    "`ridge = Ridge(alpha=alpha)`<br/>\n",
    "`ridge.fit(X_train, y_train)`<br/>\n",
    "`y_pred = ridge.predict(X_test)`<br/>\n",
    "`scores.append(ridge.score(X_test, y_test))`<br/>\n",
    "`print(scores)`<br/>\n",
    "\n",
    "#### Lasso Regression\n",
    "Lasso regression, short for Least Absolute Shrinkage and Selection Operator, is a technique used in linear regression to perform both variable selection and regularization by adding a penalty term to the ordinary least squares (OLS) method. \n",
    "\n",
    "`from sklearn.linear_model import Lasso` <br/>\n",
    "`scores = []`<br/>\n",
    "`for alpha in [0.01, 1.0, 10.0, 20.0, 50.0]:`<br/>\n",
    "`lasso = Lasso(alpha=alpha)`<br/>\n",
    "`lasso.fit(X_train, y_train)`<br/>\n",
    "`lasso_pred = lasso.predict(X_test)`<br/>\n",
    "`scores.append(lasso.score(X_test, y_test))`<br/>\n",
    "`print(scores)`<br/><br/>\n",
    "\n",
    "* It can be used to select important features of a dataset by shrinking the coefficients of less important features to zero\n",
    "\n",
    "`from sklearn.linear_model import Lasso` <br/>\n",
    "`X = diabetes_df.drop(\"glucose\", axis=1).values`<br/>\n",
    "`y = diabetes_df[\"glucose\"].values`<br/>\n",
    "`names = diabetes_df.drop(\"glucose\", axis=1).columns`<br/>\n",
    "`lasso = Lasso(alpha=0.1)`<br/>\n",
    "`lasso_coef = lasso.fit(X, y).coef_`<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic regression is a type of statistical model used for binary classification tasks in machine learning. It predicts the probability of the occurrence of a categorical dependent variable based on one or more predictor variables. `If the probability is > 0.5. the data is labeled as 1`. It creates a linear decision boundary\n",
    "\n",
    "![Image description](logistic-regression.png)<br/><br/><br/>\n",
    "\n",
    "`from sklearn.linear_model import LogisticRegression` <br/>\n",
    "`logreg = LogisticRegression()`<br/>\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0., random_state=42)`<br/>\n",
    "`logreg.fit(X_train, y_train)`<br/>\n",
    "`y_pred = logreg.predict(X_test)`<br/>\n",
    "\n",
    "We can calculate probabilities of each instance belonging to class by calling `predict_proba` method\n",
    "\n",
    "`y_pred_probs = logreg.predict_proba(X_test)[:, 1]`<br/>\n",
    "`print(y_pred_probs[0])`\n",
    "\n",
    "##### Difference between Logistic Regression and Linear Regression:\n",
    "\n",
    "Linear regression is used for predicting continuous values by establishing a linear relationship between the dependent variable and independent variables. In contrast, logistic regression is used for binary classification, aiming to predict the probability of an input belonging to a particular class.\n",
    "\n",
    "#### ROC Curve\n",
    "The `Receiver Operating Characteristic (ROC)` curve is a graphical representation used to evaluate the performance of a classification model.\n",
    "\n",
    "`from sklearn.metrics import roc_curve` <br/>\n",
    "`fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)`<br/>\n",
    "`plt.plot([0, 1], [0, 1], 'k--)`<br/>\n",
    "`plt.plot(fpr, tpr)`<br/>\n",
    "\n",
    "To quantify the performance of the model, we calculate `Area Under Curve (AUC)`.\n",
    "\n",
    "`from sklearn.metrics import roc_auc_score` <br/>\n",
    "`print(roc_auc_score(y_test, y_pred_probs))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "In supervised machine learning, `hyperparameters` are external configurations or settings for a machine learning algorithm that are not learned from the data but are set prior to the training process. These hyperparameters control various aspects of the learning process, the model's complexity, and the optimization process. <br/><br/>\n",
    "`Hyperparameter Tuning`, also known as `Hyperparameter Optimization`, is the process of finding the best set of hyperparameters for a machine learning model to optimize its performance on a specific task.\n",
    "* We can try lots of different parameter values\n",
    "* Fit them all separately\n",
    "* See how well they perform\n",
    "* Choose the best performing values\n",
    "\n",
    "<b>Grid Search</b>: \n",
    "*   This method involves specifying a set of values or ranges for each hyperparameter, and the search algorithm     exhaustively evaluates all possible combinations. It can be computationally expensive but is straightforward.\n",
    "    * `from sklearn.model_selection import GridSearchCV` <br/>\n",
    "    `kf = KFold(n_splits=5, shuffle=True, random_state=42)`<br/>\n",
    "    `param_grid = {\"alpha\" : np.arange(0.0001, 1, 10), \"solver\":[\"sag\", \"lsqr\"]}` <br/>\n",
    "    `ridge = Ridge()`<br/>\n",
    "    `ridge_cv = GridSearchCV(ridge, param_grid, cv=kf)`<br/>\n",
    "    `ridge_cv.fit(X_train, y_train)` <br/>\n",
    "    `print(ridge_cv.best_params_, ridge_cv.best_score_)`\n",
    "\n",
    "<b>Random Search</b>:\n",
    "*  Instead of exhaustively searching all combinations, random search samples hyperparameters randomly within specified ranges. It is more computationally efficient than grid search and often yields good results.\n",
    "    * `from sklearn.model_selection import RandomizedSearchCV`<br/>\n",
    "    `kf = KFold(n_splits=5, shuffle=True, random_state=42)`<br/>\n",
    "    `param_grid = {'alpha': np.arange(*0.0001, 1, 10) 'solver': ['sag', 'lsqr]}`<br/>\n",
    "    `ridge = Ridge()`<br/>\n",
    "    `ridge_cv = RandomizedSearchCV(ridge, param_grid, cv=kf, n_iter=2)`<br/>\n",
    "    `rdige_cv.fit(X_train, y_train)`<br/>\n",
    "    `print(ridge_cv.best_params_, ridge_cv.best_score_)`<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print(music_df.isna().sum().sort_values())`<br/>\n",
    "Dropping missing data\n",
    "`music_df = music_df.dropna(subset=[\"genre\",\"popularity\"])`\n",
    "`print(music_df.isna().sum().sort_values())`\n",
    "Imputint values (making educated guess of what the missing value value will be. Common to use the mean, median or mode in case of categorical values )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
