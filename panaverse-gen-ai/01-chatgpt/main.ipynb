{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "from openai.types.beta import Assistant\n",
    "from openai.types.beta.thread import Thread\n",
    "from openai.types.beta.threads.run import Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client : OpenAI = OpenAI()\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "def chat_completion(prompt : str )-> str:\n",
    "    response : ChatCompletion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "        )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "chat_completion(\"what is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat_completion()-> str:\n",
    "    completion : ChatCompletion = client.chat.completions.create(\n",
    "        model = 'gpt-3.5-turbo-1106',\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': 'You are a poetic assistant, skilled in explaining complex programming concepts in creative poetry'},\n",
    "            {'role': 'user', 'content':'Compose a poem that explains the concept of recursion in programming'}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "print(chat_completion())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the model return output in json format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even the output is in the json format, the type of output is string\n",
    "# because the output of llm is always string\n",
    "response = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo-1106',\n",
    "    response_format={'type': 'json_object'},\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": \"List of months that have 30 days\"}\n",
    "            ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(type(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even if you dont specify response format, it still will be string\n",
    "json_data = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo-1106',\n",
    "    #response_format = {'type':'json_object'},\n",
    "    messages = [\n",
    "        {'role':'system', \"content\": \"You are a helpful assistant designed to output JSON format\"},\n",
    "        {'role':'user', \"content\": \"What are the advantages to convert the output of ChatGPT into JSON format\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(json_data.choices[0].message.content)\n",
    "print(type(json_data.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom function\n",
    "import json\n",
    "\n",
    "def get_current_weather(location: str, unit: str = 'fahrenheit')-> str:\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if 'tokyo' in location.lower():\n",
    "        return json.dumps({'location':'Tokyo', \n",
    "                        'temperature': '10',\n",
    "                        'unit': 'celsius'})\n",
    "    elif 'san francisco' in location.lower():\n",
    "        return json.dumps({'location':'San francisco', \n",
    "                    'temperature': '72',\n",
    "                    'unit': 'fahrenheit'})\n",
    "    elif 'paris' in location.lower():\n",
    "        return json.dumps({'location':'Paris', \n",
    "                        'temperature': '22',\n",
    "                        'unit': 'celsius'})\n",
    "    else:\n",
    "        return json.dumps({'location':location, \n",
    "                        'temperature': 'unknown'})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conversation(main_request: str)->str:\n",
    "    #Step 1: Send the conversation and available functions to the model\n",
    "    \n",
    "    messages = [{'role': 'user','content':main_request}] #user messages list\n",
    "    \n",
    "    #along with the prompt messages, we give details about the function\n",
    "    \n",
    "    tools=[#list of function to be passed along the prompt\n",
    "        { #first function to be passed along the prompt\n",
    "            'type':'function',# define what you are passing. Here it is function\n",
    "            'function':{ #what the function is\n",
    "                'name': 'get_current_weather', #name of the function\n",
    "                'description': 'Get the current weather in a give location',\n",
    "                #what the function is doing (important for NLP to understand \n",
    "                # what the function is used for)\n",
    "                'parameters': { #what is passed as argument to function\n",
    "                    'type': 'object',\n",
    "                    'properties':{\n",
    "                        'location': {#first parameter\n",
    "                            'type': 'string', #type of location parameter\n",
    "                            'description': 'The city and state e.g San Francisco, CA',\n",
    "                            #description of the parameter\n",
    "                        },\n",
    "                        'unit': { #second parameter\n",
    "                            'type':'string', #type of the unit parameter\n",
    "                            'enum': ['celsius', 'fahrenheit']\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['location'] #required parameter (if not \n",
    "                    #provided, the function will not run)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # First request\n",
    "    \n",
    "    response: openai.ChatCompletion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-1106\", #model selection\n",
    "        messages=messages, #message list\n",
    "        tools=tools, #list of dictionary to be passed\n",
    "        tool_choice=\"auto\",  # automatically select function that is to be used\n",
    "        #if general quesiton asked, it will respond with it's own knowledge\n",
    "        #but if asked particular question, it will auto call function\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    display(\"* First Response: \", dict(response_message))\n",
    "\n",
    "    \n",
    "    tool_calls = response_message.tool_calls #additional parameter\n",
    "    display(\"First Response Tool Calls: \", tool_calls) #give details about\n",
    "    #list of function that is to be called\n",
    "    \n",
    "    #Step 2: Check if the model wanted to call a function\n",
    "    \n",
    "    if tool_calls: #if the list have any element then what to do\n",
    "        #Step 3: Call the function. The JSON response may not always be correct\n",
    "        #Be sure to handle errors\n",
    "        \n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        } #\n",
    "        \n",
    "        messages.append(response_message)  \n",
    "        # extend conversation with assistant's reply\n",
    "        \n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name #extract the name of function\n",
    "            \n",
    "            function_to_call = available_functions[function_name] # check if the\n",
    "            # present in available_functions. If so, assign the function name\n",
    "            # to the variable\n",
    "            \n",
    "            function_args = json.loads(tool_call.function.arguments) #all the \n",
    "            #parameter of the function is converted into JSON (dictionary)\n",
    "            \n",
    "            function_response = function_to_call( \n",
    "                location=function_args.get(\"location\"),#get location parameter\n",
    "                unit=function_args.get(\"unit\"), #get unit parameter\n",
    "            )\n",
    "            \n",
    "            messages.append( #response after running custom function is appended\n",
    "                            #to the messages list (thread)\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id, #which function is running and\n",
    "                    #its id\n",
    "                    \"role\": \"tool\", #role of custom function (tool)\n",
    "                    \"name\": function_name, #name of custom function \n",
    "                    \"content\": function_response, #response of the custom function\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "            \n",
    "        display(\"* Second Request Messages: \", list(messages)) \n",
    "        \n",
    "        second_response: openai.ChatCompletion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from the model where it can see the function \n",
    "        #response\n",
    "        \n",
    "        print(\"* Second Response: \", dict(second_response))\n",
    "        \n",
    "        return second_response.choices[0].message.content\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_conversation(\"What's the weather like in San Francisco, Tokyo, and Paris?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cryptocurrency_price(crypto: str)-> str:\n",
    "    '''Get crypto current price'''\n",
    "    if 'bitcoin' in crypto.lower():\n",
    "        return json.dumps({'crypto': 'Bitcoin', 'price': '40,016'})\n",
    "    elif 'ethereum'in crypto.lower():\n",
    "        return json.dumps({'crypto': 'Ethereum', 'price': '2,222'})\n",
    "    elif 'usdt' in crypto.lower():\n",
    "        return json.dumps({'crypto': 'USDT', 'price': '0.999'})\n",
    "    elif 'bnb' in crypto.lower():\n",
    "        return json.dumps({'crypto': 'BNB', 'price': '290.33'})\n",
    "    else:\n",
    "        return json.dumps({'crypto': crypto, 'price':'unknown'})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_conversation(main_request: str)->str:\n",
    "    \n",
    "    message_list = [{'role':'user', 'content':main_request}]\n",
    "    tools=[\n",
    "        {\n",
    "            'type':'function',\n",
    "            'function':{\n",
    "                'name':'get_cryptocurrency_price',\n",
    "                'description': 'Get current price of crypto currency',\n",
    "                'parameters':{\n",
    "                    'type': 'object',\n",
    "                    'properties':{\n",
    "                        'crypto':{\n",
    "                            'type': 'string',\n",
    "                            'description':'Name of the crypto currency to get current price'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['crypto']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = 'gpt-3.5-turbo-1106',\n",
    "        messages = message_list,\n",
    "        tools = tools,\n",
    "        tool_choice = 'auto'\n",
    "    )\n",
    "    \n",
    "    assistant_message = response.choices[0].message\n",
    "    tool_calls = assistant_message.tool_calls\n",
    "    \n",
    "    if tool_calls:\n",
    "        available_functions = {\n",
    "            'get_cryptocurrency_price': get_cryptocurrency_price,\n",
    "        }\n",
    "        \n",
    "        message_list.append(assistant_message)\n",
    "        \n",
    "        for tool_call in tool_calls:\n",
    "            \n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            function_response = function_to_call(\n",
    "                crypto = function_args.get('crypto')\n",
    "            )\n",
    "            \n",
    "            message_list.append({\n",
    "                'tool_call_id': tool_call.id,\n",
    "                'role':'tool',\n",
    "                'name': function_name,\n",
    "                'content': function_response\n",
    "            })\n",
    "        \n",
    "        second_response = openai.ChatCompletion.create(\n",
    "            model = 'gpt-3.5-turbo-1106',\n",
    "            messages = message_list,\n",
    "        )\n",
    "        \n",
    "        return second_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_price_conversation('What is the price of Ethereum today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_price_conversation('What is the price of Bitcoin today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_price_conversation('What is the price of PKR today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_price_conversation('What the R&B music genre means?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Assistant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The first step is to create an assistant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_json(obj):\n",
    "    display(json.loads(obj.model_dump_json()))\n",
    "\n",
    "assistant: Assistant = client.beta.assistants.create(\n",
    "    name=\"Math Tutor\",\n",
    "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    model=\"gpt-3.5-turbo-1106\"\n",
    ")\n",
    "\n",
    "show_json(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The second step is to create a thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread: Thread  = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The third step is to add the Messsage to thread\n",
    "\n",
    "In threads, we have multiple messages between the user and the assistant. We have\n",
    "to create messages, put them in a thread and then link them to the assistant\n",
    "object\n",
    "`thread` is independent of assistant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
    ")\n",
    "\n",
    "show_json(message)\n",
    "\n",
    "#we have not connected to an assistant, so assistant_id is none\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Note even though you're no longer sending the entire history each time, you still will be charged for the tokens of the entire conversation</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The fourth step is to Run the Assistant\n",
    "\n",
    "To get a completion from an Assistant for a given Thread, we must create a <b>Run</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run: Run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"Please address the user as Jane Doe. The user has a premium account.\"\n",
    ")\n",
    "\n",
    "show_json(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating `Run` is an asynchronous operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run: Run = client.beta.threads.runs.retrieve(\n",
    "  thread_id=thread.id,\n",
    "  run_id=run.id\n",
    ")\n",
    "\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_on_run(run, thread):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next step is to check the Run status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wait_on_run(run, thread)\n",
    "show_json(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The next step is display the assistant response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    ")\n",
    "\n",
    "for m in reversed(messages.data):\n",
    "    print(m.role + \": \" + m.content[0].text.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name = 'Math Tutor',\n",
    "    instructions= 'You are a personal math tutor. Write and run code to answer math questions',\n",
    "    tools = [{'type':'code_interpreter'}],\n",
    "    model = 'gpt-3.5-turbo-1106'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_2nwAuga7BbPvbxenlLG6hZRB', created_at=1706276816, metadata={}, object='thread')\n"
     ]
    }
   ],
   "source": [
    "thread = client.beta.threads.create()\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThreadMessage(id='msg_3DZp65mOVv4wwMrm0d6Vg4PR', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Solve this problem: 3x + 11 = 14'), type='text')], created_at=1706276816, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_2nwAuga7BbPvbxenlLG6hZRB')\n"
     ]
    }
   ],
   "source": [
    "message = client.beta.threads.messages.create( #we have to give three different parameters\n",
    "    thread_id = thread.id, #thread id to which we want to link this message to\n",
    "    role = 'user',\n",
    "    content = 'Solve this problem: 3x + 11 = 14'\n",
    ")\n",
    "\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    #we have to pass two parameters\n",
    "    #1) thread id 2) assistant id\n",
    "    thread_id = thread.id,\n",
    "    assistant_id= assistant.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:Solve this problem: 3x + 11 = 14\n",
      "assistant:The solution to the equation 3x + 11 = 14 is x = 1.\n"
     ]
    }
   ],
   "source": [
    "run = client.beta.threads.runs.retrieve(\n",
    "    thread_id = thread.id,\n",
    "    run_id = run.id,\n",
    ")\n",
    "\n",
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id = thread.id,\n",
    ")\n",
    "\n",
    "for message in reversed(messages.data): #we want to get the most oldest message\n",
    "    #printed out first and then the most latest message from the assistatn\n",
    "    print(message.role + \":\" + message.content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload Files to AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-knwRHHW61bR32yPEzwquhhE0', bytes=48802, created_at=1706278204, filename='zia_profile.pdf', object='file', purpose='assistants', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "file = client.files.create(\n",
    "    file=open(\"zia_profile.pdf\", \"rb\"),\n",
    "    purpose='assistants'\n",
    ")\n",
    "\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Create the assistant`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant: Assistant = client.beta.assistants.create(\n",
    "    name=\"Student Support Assistant\",\n",
    "    instructions=\"You are a student support chatbot. Use your knowledge base to best respond to student queries about Zia U. Khan.\",\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    tools=[{\"type\": \"retrieval\"}],\n",
    "    file_ids=[file.id]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Create a thread`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_6gyxd260CTtNcYA57ti4Th4R', created_at=1706278233, metadata={}, object='thread')\n"
     ]
    }
   ],
   "source": [
    "thread: Thread  = client.beta.threads.create()\n",
    "\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Create a message`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"When and which city Zia U. Khan was born? What was his first job?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Run the assistant`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "run: Run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"Please address the user as Pakistani. The user is the student of PIAIC.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Display the assistant response`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: When and which city Zia U. Khan was born? What was his first job?\n"
     ]
    }
   ],
   "source": [
    "run: Run = client.beta.threads.runs.retrieve(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    ")\n",
    "\n",
    "for m in reversed(messages.data):\n",
    "    print(m.role + \": \" + m.content[0].text.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
